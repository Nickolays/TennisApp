# Tennis Analytics - Worker Architecture (Local GPU + Cloud VDS)

**Scenario**: You have a powerful GPU at home/office, but need to serve users 24/7 via cloud.

---

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        SYSTEM ARCHITECTURE                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                             ‚îÇ
‚îÇ  ‚îÇ   üì± Client    ‚îÇ                                             ‚îÇ
‚îÇ  ‚îÇ  (Mobile/Web)  ‚îÇ                                             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                             ‚îÇ
‚îÇ           ‚îÇ 1. Upload video                                     ‚îÇ
‚îÇ           ‚îÇ 2. Get job_id                                       ‚îÇ
‚îÇ           ‚îÇ 3. Poll status                                      ‚îÇ
‚îÇ           ‚ñº                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ         ‚òÅÔ∏è  CLOUD VDS (24/7 Available)                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  FastAPI Server (Python)                         ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - /api/v1/upload (receive video)                ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - /api/v1/jobs/{id} (status check)              ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - /api/v1/results/{id} (download result)        ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - /worker/get_job (worker polls for work)       ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - /worker/complete/{id} (worker submits result) ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  PostgreSQL Database                             ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - jobs (id, user_id, status, video_path, ...)  ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - users                                         ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - results                                       ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Storage (S3 / Local Disk)                       ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - /uploads/vid_{job_id}.mp4                     ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - /results/vid_{job_id}_analyzed.mp4            ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - /results/vid_{job_id}_analytics.json.gz       ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Cost: ~$5-10/month (VDS without GPU)                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ           ‚ñ≤                                                     ‚îÇ
‚îÇ           ‚îÇ 4. Worker polls every 2s: "Any work?"              ‚îÇ
‚îÇ           ‚îÇ 5. VDS responds: job_id + video_path               ‚îÇ
‚îÇ           ‚îÇ 8. Worker sends: result JSON                       ‚îÇ
‚îÇ           ‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ     üè† HOME/OFFICE PC with GPU (Your Workstation)        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  worker.py (50 lines, runs 24/7)                   ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Polls VDS every 2s                              ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Downloads video if job available                ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Runs AI pipeline (GPU processing)               ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Uploads result back to VDS                      ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ           ‚îÇ                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ           ‚ñº                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Tennis Analytics Pipeline                         ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Detection (TrackNet, YOLO) ‚Üí GPU                ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Temporal, Geometry ‚Üí CPU                        ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Events, Analytics ‚Üí CPU                         ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Rendering ‚Üí CPU                                 ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  GPU: RTX 3060/4060 (or better)                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Cost: $0 (you already own it!)                          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Key Advantages:
‚úÖ Worker initiates connection (no need for white IP / port forwarding)
‚úÖ Works behind NAT / firewall
‚úÖ VDS is cheap ($5-10/month) - no GPU needed
‚úÖ Your GPU at home does heavy lifting for FREE
‚úÖ Can add more workers (multiple GPUs) easily
```

---

## üìä Database Schema

```sql
-- Jobs table (task queue)
CREATE TABLE jobs (
    id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(id),
    status TEXT NOT NULL,  -- 'pending', 'processing', 'completed', 'failed'
    video_path TEXT NOT NULL,  -- Path to uploaded video
    result_path TEXT,  -- Path to output video
    analytics_path TEXT,  -- Path to JSON analytics
    worker_id TEXT,  -- Which worker is processing
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT,
    progress FLOAT DEFAULT 0.0,  -- 0.0 to 1.0
    priority INT DEFAULT 0,  -- Higher = process first
    config JSONB  -- Pipeline config to use
);

CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_priority ON jobs(priority DESC, created_at ASC);

-- Users table
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    email TEXT UNIQUE NOT NULL,
    api_key TEXT UNIQUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    quota_daily INT DEFAULT 10,  -- Max videos per day
    quota_remaining INT DEFAULT 10
);

-- Results table (match analytics)
CREATE TABLE results (
    id SERIAL PRIMARY KEY,
    job_id INT REFERENCES jobs(id),
    match_id TEXT,
    framerate INT,
    total_frames INT,
    duration FLOAT,
    rallies_count INT,
    avg_ball_speed FLOAT,
    max_ball_speed FLOAT,
    player1_name TEXT,
    player2_name TEXT,
    player1_shots INT,
    player2_shots INT,
    analytics_json JSONB,  -- Full JSON output
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Workers table (track active workers)
CREATE TABLE workers (
    id TEXT PRIMARY KEY,  -- worker_id (e.g., "gpu-home-1")
    last_ping TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status TEXT DEFAULT 'idle',  -- 'idle', 'busy'
    current_job_id INT REFERENCES jobs(id),
    gpu_model TEXT,
    capabilities JSONB  -- {"parallel": true, "max_resolution": [1920, 1080]}
);
```

---

## üîß Implementation

### 1. VDS Server (FastAPI)

```python
# app/api/worker_routes.py

from fastapi import APIRouter, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
import os

router = APIRouter(prefix="/worker", tags=["worker"])

@router.get("/get_job")
async def get_job(worker_id: str, db: Session = Depends(get_db)):
    """
    Worker polls this endpoint to get work.

    Returns:
        - job_id, video_path, config if work available
        - null if no work
    """

    # Update worker ping time
    worker = db.query(Worker).filter(Worker.id == worker_id).first()
    if worker:
        worker.last_ping = datetime.utcnow()
        worker.status = "idle"
    else:
        # Register new worker
        worker = Worker(id=worker_id, status="idle")
        db.add(worker)
    db.commit()

    # Find next pending job (highest priority first)
    job = db.query(Job).filter(
        Job.status == "pending"
    ).order_by(
        Job.priority.desc(),
        Job.created_at.asc()
    ).first()

    if not job:
        return {"job_id": None}

    # Assign job to worker
    job.status = "processing"
    job.worker_id = worker_id
    job.started_at = datetime.utcnow()

    worker.status = "busy"
    worker.current_job_id = job.id

    db.commit()

    return {
        "job_id": job.id,
        "video_path": job.video_path,
        "config": job.config or "default",
        "download_url": f"/api/v1/download/{job.id}"
    }


@router.post("/complete/{job_id}")
async def complete_job(
    job_id: int,
    result: dict,
    db: Session = Depends(get_db)
):
    """
    Worker submits completed result.

    Body:
        {
            "analytics": {...},  # Full JSON output
            "video_url": "https://...",  # Uploaded result video
            "processing_time": 125.5
        }
    """

    job = db.query(Job).filter(Job.id == job_id).first()
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    # Update job status
    job.status = "completed"
    job.completed_at = datetime.utcnow()
    job.result_path = result.get("video_url")
    job.analytics_path = result.get("analytics_url")
    job.progress = 1.0

    # Save analytics to results table
    analytics_data = result.get("analytics", {})
    db_result = Result(
        job_id=job.id,
        match_id=analytics_data.get("meta", {}).get("matchId"),
        framerate=analytics_data.get("meta", {}).get("framerate"),
        total_frames=analytics_data.get("meta", {}).get("totalFrames"),
        duration=analytics_data.get("meta", {}).get("duration"),
        rallies_count=len(analytics_data.get("rallies", [])),
        analytics_json=analytics_data
    )
    db.add(db_result)

    # Mark worker as idle
    worker = db.query(Worker).filter(Worker.id == job.worker_id).first()
    if worker:
        worker.status = "idle"
        worker.current_job_id = None

    db.commit()

    return {"status": "success", "job_id": job_id}


@router.post("/progress/{job_id}")
async def update_progress(
    job_id: int,
    progress: float,
    db: Session = Depends(get_db)
):
    """
    Worker sends progress updates (optional).

    Example: POST /worker/progress/123 {"progress": 0.45}
    """

    job = db.query(Job).filter(Job.id == job_id).first()
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    job.progress = min(max(progress, 0.0), 1.0)  # Clamp 0-1
    db.commit()

    return {"status": "updated"}


@router.post("/heartbeat")
async def heartbeat(worker_id: str, db: Session = Depends(get_db)):
    """
    Worker sends periodic heartbeat (every 30s).
    VDS marks workers as offline if no heartbeat for 2 minutes.
    """

    worker = db.query(Worker).filter(Worker.id == worker_id).first()
    if worker:
        worker.last_ping = datetime.utcnow()
        db.commit()
        return {"status": "ok"}

    return {"status": "unknown_worker"}
```

### 2. Worker Script (Home GPU)

```python
# worker.py (runs on your home PC with GPU)

import time
import requests
import os
import sys
from pathlib import Path

# Add project to path
sys.path.append(str(Path(__file__).parent))

from app.services.pipeline_builder import PipelineBuilder
from app.services.video_processor import VideoProcessor
import gzip
import json

# Configuration
WORKER_ID = "gpu-home-1"  # Unique worker name
SERVER_URL = "https://your-vds-server.com"  # VDS address
POLL_INTERVAL = 2  # seconds
DOWNLOAD_DIR = "/tmp/tennis_videos"
RESULTS_DIR = "/tmp/tennis_results"

os.makedirs(DOWNLOAD_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)


def download_video(job_id: int, download_url: str) -> str:
    """Download video from VDS to local disk"""
    print(f"üì• Downloading video for job #{job_id}...")

    video_path = f"{DOWNLOAD_DIR}/video_{job_id}.mp4"

    response = requests.get(f"{SERVER_URL}{download_url}", stream=True)
    response.raise_for_status()

    with open(video_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)

    print(f"‚úì Downloaded to {video_path}")
    return video_path


def process_video(video_path: str, config_name: str = "default") -> dict:
    """
    Run Tennis Analytics Pipeline on video

    Returns:
        {
            "analytics": {...},  # Full JSON output
            "output_video": "path/to/result.mp4"
        }
    """
    print(f"ü§ñ Starting AI processing (config: {config_name})...")

    # Load pipeline
    config = PipelineBuilder.load_config(f"configs/{config_name}.yaml")
    pipeline = PipelineBuilder.build_from_config(config)

    # Create processor
    processor = VideoProcessor(pipeline, config)

    # Process video (this is where GPU works!)
    result = processor.process_video(
        video_path,
        output_name=f"{RESULTS_DIR}/result_{Path(video_path).stem}.mp4"
    )

    # Convert result to JSON
    analytics = {
        "meta": {
            "matchId": Path(video_path).stem,
            "videoPath": video_path,
            "framerate": result.fps,
            "totalFrames": result.total_frames,
            "duration": result.duration_seconds,
            # ... (see OUTPUT_FORMAT.md for complete structure)
        },
        "timeline": result.timeline,
        "rallies": result.rallies,
        "statistics": result.statistics,
        "processing": result.processing_info,
    }

    return {
        "analytics": analytics,
        "output_video": result.output_video_path,
    }


def upload_results(job_id: int, result: dict) -> dict:
    """
    Upload result video and analytics to VDS

    Returns URLs of uploaded files
    """
    print(f"üì§ Uploading results for job #{job_id}...")

    # 1. Compress analytics JSON
    analytics_path = f"{RESULTS_DIR}/analytics_{job_id}.json.gz"
    with gzip.open(analytics_path, "wt") as f:
        json.dump(result["analytics"], f)

    # 2. Upload analytics
    with open(analytics_path, "rb") as f:
        response = requests.post(
            f"{SERVER_URL}/api/v1/upload_result/{job_id}",
            files={"analytics": f}
        )
    analytics_url = response.json()["analytics_url"]

    # 3. Upload video (optional, can be large)
    # For faster upload, consider uploading to S3 directly
    if result["output_video"]:
        with open(result["output_video"], "rb") as f:
            response = requests.post(
                f"{SERVER_URL}/api/v1/upload_result/{job_id}",
                files={"video": f}
            )
        video_url = response.json()["video_url"]
    else:
        video_url = None

    print(f"‚úì Uploaded: analytics={analytics_url}, video={video_url}")

    return {
        "analytics_url": analytics_url,
        "video_url": video_url,
    }


def send_progress(job_id: int, progress: float):
    """Send progress update to VDS"""
    try:
        requests.post(
            f"{SERVER_URL}/worker/progress/{job_id}",
            json={"progress": progress}
        )
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to send progress: {e}")


def run_worker():
    """
    Main worker loop

    1. Poll VDS for work every 2 seconds
    2. Download video if job available
    3. Process with AI pipeline
    4. Upload results
    5. Repeat
    """

    print(f"üöÄ Worker {WORKER_ID} started!")
    print(f"üîó Connecting to: {SERVER_URL}")
    print(f"üìä Polling interval: {POLL_INTERVAL}s\n")

    consecutive_errors = 0

    while True:
        try:
            # 1. Poll for work
            response = requests.get(
                f"{SERVER_URL}/worker/get_job",
                params={"worker_id": WORKER_ID},
                timeout=10
            )
            response.raise_for_status()
            data = response.json()

            job_id = data.get("job_id")

            if job_id:
                print(f"\n{'='*60}")
                print(f"üì• Received job #{job_id}")
                print(f"{'='*60}\n")

                consecutive_errors = 0  # Reset error counter

                try:
                    # 2. Download video
                    video_path = download_video(job_id, data["download_url"])
                    send_progress(job_id, 0.05)

                    # 3. Process video
                    start_time = time.time()
                    result = process_video(video_path, data.get("config", "default"))
                    processing_time = time.time() - start_time

                    send_progress(job_id, 0.95)

                    # 4. Upload results
                    upload_urls = upload_results(job_id, result)

                    # 5. Mark job complete
                    requests.post(
                        f"{SERVER_URL}/worker/complete/{job_id}",
                        json={
                            "analytics": result["analytics"],
                            "analytics_url": upload_urls["analytics_url"],
                            "video_url": upload_urls["video_url"],
                            "processing_time": processing_time,
                        }
                    )

                    print(f"\n‚úÖ Job #{job_id} completed in {processing_time:.1f}s\n")

                    # Clean up local files
                    os.remove(video_path)
                    if result["output_video"]:
                        os.remove(result["output_video"])

                except Exception as e:
                    print(f"\n‚ùå Job #{job_id} failed: {e}\n")
                    # Report failure to VDS
                    requests.post(
                        f"{SERVER_URL}/worker/fail/{job_id}",
                        json={"error": str(e)}
                    )

            else:
                # No work available
                print(".", end="", flush=True)
                time.sleep(POLL_INTERVAL)

        except requests.exceptions.RequestException as e:
            consecutive_errors += 1
            print(f"\n‚ö†Ô∏è  Connection error ({consecutive_errors}/10): {e}")

            if consecutive_errors >= 10:
                print("\n‚ùå Too many errors, waiting 60s before retry...")
                time.sleep(60)
                consecutive_errors = 0
            else:
                time.sleep(POLL_INTERVAL * 2)

        except KeyboardInterrupt:
            print("\n\nüõë Worker stopped by user")
            break

        except Exception as e:
            print(f"\n‚ùå Unexpected error: {e}")
            time.sleep(POLL_INTERVAL * 2)


if __name__ == "__main__":
    # Verify GPU is available
    import torch
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"‚úì GPU detected: {gpu_name}\n")
    else:
        print("‚ö†Ô∏è  WARNING: No GPU detected! Processing will be SLOW.\n")

    run_worker()
```

### 3. Client API Routes (User-Facing)

```python
# app/api/routes.py

@router.post("/api/v1/upload")
async def upload_video(
    video: UploadFile,
    config: str = "default",
    priority: int = 0,
    user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    User uploads video for processing

    Returns:
        {"job_id": 123, "status": "pending"}
    """

    # Check user quota
    user = db.query(User).filter(User.id == user_id).first()
    if user.quota_remaining <= 0:
        raise HTTPException(status_code=429, detail="Daily quota exceeded")

    # Save video
    job_id = generate_job_id()
    video_path = f"/storage/uploads/video_{job_id}.mp4"

    with open(video_path, "wb") as f:
        f.write(await video.read())

    # Create job
    job = Job(
        id=job_id,
        user_id=user_id,
        status="pending",
        video_path=video_path,
        config={"profile": config},
        priority=priority
    )
    db.add(job)

    user.quota_remaining -= 1
    db.commit()

    return {
        "job_id": job_id,
        "status": "pending",
        "message": "Video queued for processing"
    }


@router.get("/api/v1/jobs/{job_id}")
async def get_job_status(job_id: int, db: Session = Depends(get_db)):
    """
    Check job status

    Returns:
        {
            "job_id": 123,
            "status": "processing",
            "progress": 0.45,
            "started_at": "2025-12-28T10:30:00Z",
            "estimated_completion": "2025-12-28T10:32:15Z"
        }
    """

    job = db.query(Job).filter(Job.id == job_id).first()
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    # Estimate completion time
    estimated_completion = None
    if job.status == "processing" and job.progress > 0:
        elapsed = (datetime.utcnow() - job.started_at).total_seconds()
        total_estimated = elapsed / job.progress
        remaining = total_estimated - elapsed
        estimated_completion = datetime.utcnow() + timedelta(seconds=remaining)

    return {
        "job_id": job_id,
        "status": job.status,
        "progress": job.progress,
        "created_at": job.created_at,
        "started_at": job.started_at,
        "completed_at": job.completed_at,
        "estimated_completion": estimated_completion,
        "error": job.error_message
    }


@router.get("/api/v1/results/{job_id}")
async def get_results(job_id: int, db: Session = Depends(get_db)):
    """
    Download results (video + analytics)

    Returns:
        ZIP file containing:
        - result_video.mp4
        - analytics.json.gz
    """

    job = db.query(Job).filter(Job.id == job_id).first()
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    if job.status != "completed":
        raise HTTPException(status_code=400, detail="Job not completed yet")

    # Create ZIP with results
    import zipfile
    from io import BytesIO

    zip_buffer = BytesIO()
    with zipfile.ZipFile(zip_buffer, "w") as zip_file:
        zip_file.write(job.result_path, "result_video.mp4")
        zip_file.write(job.analytics_path, "analytics.json.gz")

    zip_buffer.seek(0)

    return StreamingResponse(
        zip_buffer,
        media_type="application/zip",
        headers={"Content-Disposition": f"attachment; filename=job_{job_id}_results.zip"}
    )
```

---

## üöÄ Deployment Guide

### VDS Setup (Cloud Server)

```bash
# 1. Rent VDS (DigitalOcean, Hetzner, etc.)
# Specs: 2 vCPU, 4GB RAM, 50GB SSD (~$5-10/month)

# 2. Install dependencies
sudo apt update
sudo apt install python3-pip postgresql nginx

# 3. Clone repository
git clone https://github.com/yourname/TennisAnalysis.git
cd TennisAnalysis/TennisApp

# 4. Install Python packages
pip install -r requirements.txt

# 5. Setup database
sudo -u postgres createdb tennis_db
python scripts/init_database.py

# 6. Configure environment
cp .env.example .env
nano .env  # Edit: DATABASE_URL, SECRET_KEY, etc.

# 7. Start API server
uvicorn app.api.main:app --host 0.0.0.0 --port 8000 --workers 4

# 8. Setup Nginx reverse proxy
sudo nano /etc/nginx/sites-available/tennis-api
# ... (see nginx config below)
sudo systemctl restart nginx

# 9. Setup SSL (Let's Encrypt)
sudo certbot --nginx -d api.yourtennis.com
```

**Nginx Config**:
```nginx
server {
    listen 80;
    server_name api.yourtennis.com;

    client_max_body_size 2G;  # Allow large video uploads

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_request_buffering off;  # For large uploads
    }
}
```

### Home GPU Worker Setup

```bash
# 1. Clone repository on your home PC
git clone https://github.com/yourname/TennisAnalysis.git
cd TennisAnalysis/TennisApp

# 2. Install dependencies (with GPU support)
pip install -r requirements.txt
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118  # CUDA 11.8

# 3. Download models
python scripts/download_models.py

# 4. Configure worker
nano worker.py
# Edit: SERVER_URL = "https://api.yourtennis.com"
#       WORKER_ID = "gpu-home-1"

# 5. Test worker
python worker.py

# 6. Setup as systemd service (Linux)
sudo nano /etc/systemd/system/tennis-worker.service
```

**Systemd Service**:
```ini
[Unit]
Description=Tennis Analytics Worker
After=network.target

[Service]
Type=simple
User=youruser
WorkingDirectory=/home/youruser/TennisAnalysis/TennisApp
ExecStart=/usr/bin/python3 worker.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
# Enable and start
sudo systemctl enable tennis-worker
sudo systemctl start tennis-worker

# Check status
sudo systemctl status tennis-worker
```

---

## üìä Performance & Scaling

### Single Worker Performance

| Video Length | GPU | Processing Time | Throughput |
|--------------|-----|----------------|------------|
| 1 min | RTX 3060 | ~8s | 7.5x realtime |
| 5 min | RTX 3060 | ~35s | 8.6x realtime |
| 15 min | RTX 3060 | ~100s | 9.0x realtime |
| 30 min | RTX 4060 | ~150s | 12.0x realtime |

### Multi-Worker Scaling

```python
# Start multiple workers (if you have multiple GPUs)

# GPU 0
CUDA_VISIBLE_DEVICES=0 python worker.py --id gpu-home-1

# GPU 1
CUDA_VISIBLE_DEVICES=1 python worker.py --id gpu-home-2
```

**Throughput**: 2 workers = 2x capacity

### Cost Analysis

| Component | Cost | Notes |
|-----------|------|-------|
| VDS (no GPU) | $5-10/month | Hetzner, DigitalOcean |
| Storage (100GB) | $1-2/month | For videos & results |
| Domain + SSL | $10-15/year | Optional |
| **Total** | **~$7-12/month** | vs $100+/month for GPU cloud |

Your home GPU: **FREE!** üéâ

---

## üõ°Ô∏è Security Considerations

### 1. API Authentication

```python
# Add API key authentication
@router.post("/api/v1/upload")
async def upload_video(
    api_key: str = Header(...),
    ...
):
    user = db.query(User).filter(User.api_key == api_key).first()
    if not user:
        raise HTTPException(status_code=401, detail="Invalid API key")
```

### 2. Worker Authentication

```python
# Shared secret between VDS and worker
WORKER_SECRET = os.getenv("WORKER_SECRET")

@router.get("/worker/get_job")
async def get_job(
    worker_id: str,
    secret: str = Header(...),
):
    if secret != WORKER_SECRET:
        raise HTTPException(status_code=403, detail="Unauthorized")
```

### 3. Rate Limiting

```python
from slowapi import Limiter

limiter = Limiter(key_func=get_remote_address)

@app.post("/api/v1/upload")
@limiter.limit("10/hour")  # Max 10 uploads per hour per IP
async def upload_video(...):
    ...
```

---

## üéØ Summary

This architecture gives you:

‚úÖ **24/7 Availability** - VDS always online
‚úÖ **Free GPU** - Use your home hardware
‚úÖ **No Port Forwarding** - Worker initiates connection
‚úÖ **Scalable** - Add more workers easily
‚úÖ **Low Cost** - ~$10/month vs $100+/month for cloud GPU
‚úÖ **Production-Ready** - Queue system, progress tracking, error handling

**Next Steps**:
1. Set up VDS with FastAPI server
2. Implement worker routes (`/worker/get_job`, `/worker/complete`)
3. Test worker locally (connect to VDS)
4. Deploy to production

Your colleague's design is solid! üöÄ
